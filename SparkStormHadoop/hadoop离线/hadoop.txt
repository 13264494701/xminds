讲述HDFS上传文件和读文件的流程

HDFS 上传流程   
过程解析：详解
这里描述的 是一个256M的文件上传过程 
① 由客户端 向 NameNode节点节点 发出请求
②NameNode 向Client返回可以可以存数据的 DataNode 这里遵循  机架感应  原则

③客户端 首先 根据返回的信息 先将 文件分块（Hadoop2.X版本 每一个block为 128M 而之前的版本为 64M）
④然后通过那么Node返回的DataNode信息 直接发送给DataNode 并且是 流式写入  同时 会复制到其他两台机器
⑤dataNode 向 Client通信 表示已经传完 数据块 同时向NameNode报告
⑥依照上面（④到⑤）的原理将 所有的数据块都上传结束 向 NameNode 报告 表明 已经传完所有的数据块 

这样 整个HDFS上传流程就 走完了 （来自csdn Only丶爱你）


相关文章：
HDFS文件读写及准确性介绍
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6966

------------------------------------------------------------
Hadoop学习总结：HDFS读写过程解析                           -
HDFS追本溯源：租约，读写过程的容错处理及NN的主要数据结构   -
http://www.aboutyun.com/forum.php?mod=viewthread&tid=17620 -
------------------------------------------------------------

HDFS在上传文件的时候，如果其中一个块突然损坏了怎么办
其中一个块坏了，只要有其它块存在，会自动检测还原。

NameNode的作用
namenode总体来说是管理和记录恢复功能。
比如管理datanode，保持心跳，如果超时则排除。
对于上传文件都有镜像images和edits,这些可以用来恢复。更多：
深度了解namenode---其 内部关键数据结构原理简介
http://www.aboutyun.com/forum.php?mod=viewthread&tid=7388


NameNode在启动的时候会做哪些操作
NameNode启动的时候，会加载fsimage

更多参考下面内容
NameNode启动过程fsimage加载过程

Fsimage加载过程完成的操作主要是为了：
1.         从fsimage中读取该HDFS中保存的每一个目录和每一个文件
2.         初始化每个目录和文件的元数据信息
3.         根据目录和文件的路径，构造出整个namespace在内存中的镜像
4.         如果是文件，则读取出该文件包含的所有blockid，并插入到BlocksMap中。
整个加载流程如下图所示：
  

如上图所示，namenode在加载fsimage过程其实非常简单，就是从fsimage中不停的顺序读取文件和目录的元数据信息，并在内存中构建整个namespace，同时将每个文件对应的blockid保存入BlocksMap中，此时BlocksMap中每个block对应的datanodes列表暂时为空。当fsimage加载完毕后，整个HDFS的目录结构在内存中就已经初始化完毕，所缺的就是每个文件对应的block对应的datanode列表信息。这些信息需要从datanode的blockReport中获取，所以加载fsimage完毕后，namenode进程进入rpc等待状态，等待所有的datanodes发送blockReports。



NameNode的HA
NameNode的HA一个备用，一个工作，且一个失败后，另一个被激活。他们通过journal node来实现共享数据。
更多
Hadoop之NameNode+ResourceManager高可用原理分析
http://www.aboutyun.com/forum.php?mod=viewthread&tid=16024

Hadoop常见 HA方案 及如何解决HA
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6724


NameNode和DataNode之间有哪些操作
这个问题有些歧义。操作具体可以查看hadoop命令，应该超不出命令汇总
Hadoop Shell命令字典（可收藏）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6983

hadoop高级命令详解
http://www.aboutyun.com/forum.php?mod=viewthread&tid=14829



Hadoop的作业提交流程
Hadoop2.x Yarn作业提交（客户端）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=9498
Hadoop2.x Yarn作业提交（服务端）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=9496

更多：
hadoop作业提交脚本分析（1）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6954

hadoop作业提交脚本分析（2）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6956


Hadoop怎么分片
如何让hadoop按文件分片
http://www.aboutyun.com/forum.php?mod=viewthread&tid=14549

Hadoop分块与分片
http://www.aboutyun.com/blog-5994-697.html

如何减少Hadoop Map端到Reduce端的数据传输量
减少传输量，可以让map处理完，让同台的reduce直接处理，理想情况下，没有数据传输。

Hadoop的Shuffle
彻底了解mapreduce核心Shuffle--解惑各种mapreduce问题
http://www.aboutyun.com/forum.php?mod=viewthread&tid=7078

hadoop代码笔记 Mapreduce shuffle过程之Map输出过程((1)
http://www.aboutyun.com/forum.php?mod=viewthread&tid=10335


HMaster的作用
hmaster的作用
为region server分配region.
负责region server的负载均衡。
发现失效的region server并重新分配其上的region.
Gfs上的垃圾文件回收。
处理schema更新请求。
更多
region server and hmaster server

HBase的操作数据的步骤
Hbase写数据，存数据，读数据的详细过程
http://www.aboutyun.com/forum.php?mod=viewthread&tid=10886

Client写入 -> 存入MemStore，一直到MemStore满 -> Flush成一个StoreFile，直至增长到一定阈值 -> 出发Compact合并操作 -> 多个StoreFile合并成一个StoreFile，同时进行版本合并和数据删除 -> 当StoreFiles Compact后，逐步形成越来越大的StoreFile -> 单个StoreFile大小超过一定阈值后，触发Split操作，把当前Region Split成2个Region，Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上由此过程可知，HBase只是增加数据，有所得更新和删除操作，都是在Compact阶段做的，所以，用户写操作只需要进入到内存即可立即返回，从而保证I/O高性能。

client->zookeeper->.ROOT->.META-> 用户数据表zookeeper记录了.ROOT的路径信息（root只有一个region），.ROOT里记录了.META的region信息， （.META可能有多个region），.META里面记录了region的信息。

-----------------------------------

6.     HDFS的安全模式
          Namenode 启动后会进入一个称为安全模式的特殊状态。处于安全模式 的Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode上。


7.      读过程分析
•使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；
• Namenode会视情况返回文件的部分或者全部block列表，对于每个block，Namenode都会返回有该block拷贝的DataNode地址；
•客户端开发库Client会选取离客户端最接近的DataNode来读取block；如果客户端本身就是DataNode,那么将从本地直接获取数据.
•读取完当前block的数据后，关闭与当前的DataNode连接，并为读取下一个block寻找最佳的DataNode；
•当读完列表的block后，且文件读取还没有结束，客户端开发库会继续向Namenode获取下一批的block列表。
•读取完一个block都会进行checksum验证，如果读取datanode时出现错误，客户端会通知Namenode，然后再从下一个拥有该block拷贝的datanode继续读。

8.     写过程流程分析
•使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；
•Namenode会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件 创建一个记录，否则会让客户端抛出异常；
•当客户端开始写入文件的时候，会将文件切分成多个packets，并在内部以数据队列"data queue"的形式管理这些packets，并向Namenode申请新的blocks，获取用来存储replicas的合适的datanodes列表， 列表的大小根据在Namenode中对replication的设置而定。
•开始以pipeline（管道）的形式将packet写入所有的replicas中。把packet以流的方式写入第一个datanode， 该datanode把该packet存储之后，再将其传递给在此pipeline中的下一个datanode，直到最后一个datanode，这种写数据 的方式呈流水线的形式。
•最后一个datanode成功存储之后会返回一个ack packet，在pipeline里传递至客户端，在客户端的开发库内部维护着"ack queue"，成功收到datanode返回的ackpacket后会从"ackqueue"移除相应的packet。
•如果传输过程中，有某个datanode出现了故障，那么当前的pipeline会被关闭，出现故障的datanode会从当前的pipeline中移除，剩余的block会继续剩下的datanode中继续以pipeline的形式传输，同时Namenode会分配一个新的datanode，保持replicas设定的数量。


流水线复制
              当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个

