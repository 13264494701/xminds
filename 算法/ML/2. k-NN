

一、
（1）KNN 和Kmeans缺点
    都属于惰性学习机制，需要大量的计算距离过程，速度慢的可以（但是都有相应的优化方法）。
（2）KNN
    KNN不需要进行训练，只要对于一个陌生的点利用离其最近的K个点的标签判断其结果。KNN相当于多数表决，也就等价于经验最小化。而KNN的优化方式就是用Kd树来实现。
（3）Kmean
    要求自定义K个聚类中心，然后人为的初始化聚类中心，通过不断增加新点变换中心位置得到最终结果。Kmean的缺点可以用Kmean++方法进行一些解决（思想是使得初始聚类中心之间的距离最大化）
二、
    1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；

    2. 可用于非线性分类；

    3. 训练时间复杂度为O(n)；

    4. 准确度高，对数据没有假设，对outlier不敏感；

    缺点：

    1. 计算量大；

    2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；

    3. 需要大量的内存；

三、

    KNN算法的结果很大程度取决于K的选择。

         在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：



    同时，KNN通过依据k个对象中占优的类别进行决策，而不是单一的对象类别决策。这两点就是KNN算法的优势。

       接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：

    1）计算测试数据与各个训练数据之间的距离；

    2）按照距离的递增关系进行排序；

    3）选取距离最小的K个点；

    4）确定前K个点所在类别的出现频率；

    5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

