在统计模式识分类问题中，当先验概率未知时，可以使用()?
	在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。
	1. p(y)已知，直接使用贝叶斯公式求后验概率即可；
	2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。
	而最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。

三个问题
评估问题：即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型，使用向前向后算法
训练问题：即参数估计，是一种无监督的训练方法，主要通过EM迭代实现，Baum-Welch算法解决
预测问题：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。
如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。

EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法
维特比算法： 用动态规划解决HMM的预测问题，不是参数估计
前向后向：用来算概率
极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数




在()情况下,用分支定界法做特征选择计算量相对较少?
选用的可分性判据J对特征数目单调不减
C(n,d)>>n(n为原特征个数，d为要选出的特征个数)




用于特征降维的方法包括：
主成分分析PCA
线性判别分析LDA
深度学习，稀疏编码
矩阵奇异值分解SVD



基于二次准则函数的H-K算法较之于感知器算法的优点是()?
可以判别问题是否线性可分
其解完全适用于非线性可分的情况
其解的适应性更好





关于线性回归的描述,以下正确的有:
基本假设包括随机干扰下是均值为0的同方差正态分布
在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量
可以用DW检验残差是否存在序列相关性



1）CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ――――与HMM比较
（2）同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 --――――与MEMM比较
（3）CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。
――――与ME比较
缺点：训练代价大、复杂度高


CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优
CRF 的缺点：速度慢



时间序列模型
AR模型：自回归模型，是一种线性模型
MA模型：移动平均法模型，其中使用趋势移动平均法建立直线趋势的预测模型
ARMA模型：自回归滑动平均模型，拟合较高阶模型
GARCH模型：广义回归模型，对误差的方差建模，适用于波动性的分析和预测



概率函数
概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。
概率密度函数（p robability density function，PDF ）是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。
累积分布函数（cumulative distribution function，CDF） 能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对於所有实数x ，与pdf相对。



判别式模型常见的主要有（判别模型）：

Logistic Regression
SVM
Traditional Neural Networks
Nearest Neighbor
CRF
Linear Discriminant Analysis
Boosting
Linear Regression

产生式模型常见的主要有（生成模型）：                
       Gaussians
       Naive Bayes

       Mixtures of Multinomials
       Mixtures of Gaussians
       Mixtures of Experts
       HMMs
Sigmoidal Belief Networks, Bayesian Networks
Markov Random Fields
Latent Dirichlet Allocation



位势函数法的积累势函数K(x)的作用相当于Bayes判决中的()
后验概率
类概率密度与先验概率的乘积


影响聚类算法效果的主要原因有：（　）？
特征选取
模式相似性测度
分类准则


判断有向图有环
深度，广度优先遍历
拓扑排序
求最短路径
